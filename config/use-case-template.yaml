# Use Case Configuration Template
# Copy this file and customize for your specific use case

# ============================================================================
# USE CASE DEFINITION
# ============================================================================

use_case:
  name: "Real-time Recommendation System"  # Human-readable name
  slug: "realtime_recommendation"  # Snake_case identifier (used for module names)
  domain: "MarTech"  # MarTech, AdTech, FinTech, Healthcare, etc.
  description: "GPU-accelerated recommendation engine using collaborative filtering"

# ============================================================================
# REQUIREMENTS & GOALS
# ============================================================================

requirements:
  primary_goal: "Generate personalized product recommendations in <50ms p99 latency"
  data_scale:
    training_samples: 10_000_000
    inference_throughput: "10K requests/sec"
    features: 100
  latency_requirement:
    target_p50_ms: 20
    target_p99_ms: 50
  model_types:
    - "Collaborative Filtering"
    - "Matrix Factorization"
    - "Neural Collaborative Filtering (optional)"

# ============================================================================
# TECHNOLOGY STACK
# ============================================================================

technology_stack:
  # RAPIDS Components
  rapids:
    cudf: true  # GPU DataFrames
    cuml: true  # GPU ML algorithms
    cugraph: false  # Graph algorithms (set true if needed)
    dask_cudf: false  # Multi-GPU (set true if needed)

  # TensorRT
  tensorrt:
    enabled: false  # Enable for deep learning inference optimization
    precision: "fp16"  # fp32, fp16, int8

  # Model Framework
  model_framework: "cuML"  # cuML, XGBoost, PyTorch, TensorFlow
  model_type: "KNN"  # Specific algorithm: KNN, RandomForest, XGBoost, etc.

  # Deployment
  deployment:
    target: "single_gpu"  # single_gpu, multi_gpu, distributed
    api_framework: "fastapi"  # fastapi, flask, grpc

# ============================================================================
# DATA CONFIGURATION
# ============================================================================

data:
  # Data Sources
  sources:
    training: "s3://bucket/train/"  # Or local path, database URL, etc.
    validation: "s3://bucket/val/"
    test: "s3://bucket/test/"

  # Data Format
  format: "parquet"  # parquet, csv, json, avro

  # Features
  features:
    - "user_id"
    - "item_id"
    - "timestamp"
    - "rating"
    # Add more features as needed

  target_variable: "rating"  # For supervised learning

  # Data Splits
  splits:
    train: 0.7
    validation: 0.15
    test: 0.15

# ============================================================================
# PREPROCESSING
# ============================================================================

preprocessing:
  steps:
    - "remove_duplicates"
    - "handle_missing_values"
    - "filter_outliers"
    - "encode_categorical"
    - "normalize_features"

  missing_values:
    strategy: "drop"  # drop, fill, interpolate
    fill_value: null

  outlier_detection:
    method: "iqr"  # iqr, zscore, quantile
    threshold: 1.5

  categorical_encoding:
    method: "label"  # label, onehot
    columns:
      - "category"
      - "brand"

  normalization:
    method: "minmax"  # minmax, zscore, robust
    columns:
      - "price"
      - "popularity"

# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

feature_engineering:
  # Time-based features
  time_features:
    enabled: true
    timestamp_column: "timestamp"
    extract:
      - "hour"
      - "day_of_week"
      - "is_weekend"

  # Aggregation features
  aggregations:
    - group_by: ["user_id"]
      agg_column: "rating"
      functions: ["mean", "count", "std"]

  # Interaction features
  interactions:
    - columns: ["price", "popularity"]
      operations: ["multiply", "divide"]

  # Custom features (use-case specific)
  custom:
    - name: "user_item_frequency"
      description: "Number of times user interacted with item"
    - name: "recency_score"
      description: "Time since last interaction"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

model:
  # Model Type
  type: "KNN"  # Specific algorithm
  task: "recommendation"  # classification, regression, clustering, recommendation

  # Hyperparameters
  params:
    n_neighbors: 10
    algorithm: "brute"  # cuML KNN uses brute force
    metric: "euclidean"

  # Training Configuration
  training:
    epochs: 10  # For iterative models
    batch_size: 1024
    learning_rate: 0.001
    early_stopping_patience: 3
    validation_split: 0.2

  # Model Storage
  checkpoint_dir: "models/checkpoints"
  save_best_only: true

# ============================================================================
# INFERENCE CONFIGURATION
# ============================================================================

inference:
  batch_size: 256
  precision: "fp32"  # fp32, fp16, int8 (if TensorRT enabled)
  max_latency_ms: 50
  enable_tensorrt: false

  # API Configuration
  api:
    host: "0.0.0.0"
    port: 8000
    max_concurrent_requests: 100
    timeout_sec: 5

# ============================================================================
# GPU CONFIGURATION
# ============================================================================

gpu:
  enabled: true
  device_id: 0
  memory_fraction: 0.8  # Use 80% of GPU memory
  multi_gpu: false
  num_gpus: 1
  fallback_to_cpu: true  # Fallback if GPU unavailable

# ============================================================================
# DEPENDENCIES
# ============================================================================

dependencies:
  # Core RAPIDS (adjust cu11/cu12 based on CUDA version)
  rapids:
    - "cudf-cu12>=24.10.0"
    - "cuml-cu12>=24.10.0"
    # Add more as needed:
    # - "cugraph-cu12>=24.10.0"
    # - "dask-cudf>=24.10.0"

  # Model frameworks
  ml:
    # - "xgboost>=2.0.0"
    # - "torch>=2.0.0"
    # - "tensorflow>=2.13.0"

  # TensorRT (if enabled)
  tensorrt:
    # - "tensorrt>=8.6.0"
    # - "onnx>=1.14.0"

  # API and utilities
  api:
    - "fastapi>=0.104.0"
    - "uvicorn>=0.24.0"
    - "pydantic>=2.0.0"

  # Supporting libraries
  utils:
    - "pandas>=2.0.0"  # CPU fallback
    - "scikit-learn>=1.3.0"
    - "numpy>=1.24.0"
    - "pyyaml>=6.0"

  # Optional: HuggingFace
  huggingface:
    # - "transformers>=4.30.0"
    # - "datasets>=2.14.0"

# ============================================================================
# BENCHMARKING
# ============================================================================

benchmarking:
  enable_cpu_baseline: true

  # Data pipeline benchmarks
  data_pipeline:
    data_sizes: [10_000, 100_000, 1_000_000]
    metrics: ["load_time", "preprocess_time", "throughput"]

  # Training benchmarks
  training:
    data_sizes: [100_000, 1_000_000]
    metrics: ["training_time", "samples_per_sec", "gpu_memory_usage"]

  # Inference benchmarks
  inference:
    batch_sizes: [1, 8, 32, 128, 256]
    num_requests: 1000
    metrics: ["p50_latency", "p95_latency", "p99_latency", "throughput"]

  # Expected performance
  expected_speedup:
    data_loading: 10.0  # 10x faster than CPU
    preprocessing: 15.0
    training: 20.0
    inference: 5.0

# ============================================================================
# ENVIRONMENT
# ============================================================================

environment:
  python_version: "3.11"
  cuda_version: "12.0"

  # Development
  development:
    docker_image: "nvcr.io/nvidia/rapidsai/base:24.10-cuda12.0-py3.11"
    local_fallback: true  # Enable CPU mode for local dev

  # Production
  production:
    platform: "gcp"  # gcp, aws, azure, on-prem
    instance_type: "n1-standard-8"
    gpu_type: "T4"  # T4, V100, A100, L4
    kubernetes: false

# ============================================================================
# LOGGING & MONITORING
# ============================================================================

logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_dir: "logs"

  # Metrics tracking
  metrics:
    - "gpu_utilization"
    - "gpu_memory_usage"
    - "inference_latency"
    - "throughput"
    - "model_accuracy"
